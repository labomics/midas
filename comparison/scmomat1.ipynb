{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "os.chdir(\"/root/workspace/code/midas/\")\n",
    "from os.path import join as pj\n",
    "import argparse\n",
    "import sys\n",
    "sys.path.append(\"modules\")\n",
    "import utils\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as opt\n",
    "from torch.nn import Module, Parameter, ParameterList, ParameterDict\n",
    "from torch import softmax, log_softmax, Tensor\n",
    "import anndata as ad\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import re\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--task', type=str, default='teadog_single_full')\n",
    "parser.add_argument('--experiment', type=str, default='e0')\n",
    "parser.add_argument('--model', type=str, default='default')\n",
    "parser.add_argument('--init_model', type=str, default='sp_00001899')\n",
    "parser.add_argument('--method', type=str, default='scmomat')\n",
    "o, _ = parser.parse_known_args()  # for python interactive\n",
    "# o = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class scmomat(Module):\n",
    "    \"\"\"\\\n",
    "        Gene clusters more than cell clusters, force A_r and A_g to be sparse:\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, counts, K = 30, batch_size = 0.3, interval = 10, lr = 1e-2, lamb = 0.001, seed = None, device = torch.device('cuda')):\n",
    "        super().__init__()\n",
    "        \n",
    "        # init parameters, \n",
    "        # self.K is the number of latent dimensions\n",
    "        self.K = K\n",
    "        # latent dimensions for cells and feats\n",
    "        self.N_cell = self.K\n",
    "        self.N_feat = self.K\n",
    "        # number of batches\n",
    "        self.nbatches = counts[\"nbatches\"]\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.interval = interval\n",
    "        self.device = device\n",
    "        self.alpha = [1000, lamb * 1000]\n",
    "        \n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "        \n",
    "        # 1. load count matrices\n",
    "        self.mods = [mod for mod in counts.keys() if (mod != \"feats_name\") & (mod != \"nbatches\")]\n",
    "        self.Xs = {}\n",
    "        # mod include: RNA, ATAC, ADT, etc.\n",
    "        for mod in self.mods:\n",
    "            self.Xs[mod] = []\n",
    "            for counts_mod in counts[mod]:\n",
    "                if counts_mod is not None:\n",
    "                    self.Xs[mod].append(torch.FloatTensor(counts_mod).to(self.device))\n",
    "                else:\n",
    "                    self.Xs[mod].append(None)\n",
    "        \n",
    "        \n",
    "        # name of the features\n",
    "        if \"feats_name\" in counts.keys():\n",
    "            self.feats_name = counts[\"feats_name\"]\n",
    "        else:\n",
    "            self.feats_name = None\n",
    "\n",
    "        # put into sanity check\n",
    "        self.sanity_check()\n",
    "        \n",
    "        \n",
    "        # 2. create parameters\n",
    "        self.C_cells = ParameterDict({})\n",
    "        self.C_feats = ParameterDict({})\n",
    "        self.A_assos = ParameterDict({})\n",
    "        self.b_cells = {}\n",
    "        self.b_feats = {}\n",
    "        self.scales = {}\n",
    "        \n",
    "        # create C_cells\n",
    "        for batch in range(self.nbatches):\n",
    "            for mod in self.mods:\n",
    "                if self.Xs[mod][batch] is not None:\n",
    "                    self.C_cells[str(batch)] = Parameter(torch.rand((self.Xs[mod][batch].shape[0], self.N_cell), device = self.device))\n",
    "                    break\n",
    "        \n",
    "        # create C_feats, matrices exists for all mods\n",
    "        for mod in self.mods:\n",
    "            for batch in range(self.nbatches):\n",
    "                if self.Xs[mod][batch] is not None:\n",
    "                    self.C_feats[mod] = Parameter(torch.rand((self.Xs[mod][batch].shape[1], self.N_feat), device = self.device))\n",
    "                    break\n",
    "        \n",
    "        # create A_assos\n",
    "        self.A_assos[\"shared\"] = Parameter(torch.rand((self.N_cell, self.N_feat), device = self.device))\n",
    "        for mod in self.mods:\n",
    "            for batch in range(self.nbatches):\n",
    "                if self.Xs[mod][batch] is not None:\n",
    "                    self.A_assos[mod + \"_\" + str(batch)] = Parameter(torch.zeros((self.N_cell, self.N_feat), device = self.device))\n",
    "\n",
    "        \n",
    "        # create bias term\n",
    "        for mod in self.mods:\n",
    "            self.b_cells[mod] = {}\n",
    "            self.b_feats[mod] = {}\n",
    "            for batch in range(self.nbatches):\n",
    "                if self.Xs[mod][batch] is not None:\n",
    "                    self.b_cells[mod][batch] = torch.zeros(self.Xs[mod][batch].shape[0], 1).to(self.device)\n",
    "                    self.b_feats[mod][batch] = torch.zeros(1, self.Xs[mod][batch].shape[1]).to(self.device)\n",
    "\n",
    "                self.scales[mod] = torch.FloatTensor([1] * self.nbatches).to(self.device)\n",
    "\n",
    "        \n",
    "        self.optimizer = opt.Adam(self.parameters(), lr=lr)\n",
    "        \n",
    "    \n",
    "    def sanity_check(self):\n",
    "        print(\"Input sanity check...\")\n",
    "        # number of batches are the same\n",
    "        n_features = {}\n",
    "        for mod in self.Xs.keys():\n",
    "            # No all None modality\n",
    "            if np.all(np.array([x is None for x in self.Xs[mod]]) == True) == True:\n",
    "                raise ValueError(\"Don't have count matrix correspond to \" + mod)\n",
    "            \n",
    "            if (len(self.Xs[mod]) == self.nbatches) == False:\n",
    "                raise ValueError(\"Number of batches not match for \" + mod)\n",
    "            \n",
    "            # feature dimension should be the same\n",
    "            n_features[mod] = [x.shape[1] for x in self.Xs[mod] if x is not None]\n",
    "            if np.all(np.array(n_features[mod]) == n_features[mod][0]) == False:\n",
    "                raise ValueError(\"Number of features not match for modality \" + mod)\n",
    "\n",
    "            # number of feats_name equals to feature dimension\n",
    "            if self.feats_name is not None:\n",
    "                if self.feats_name[mod].shape[0] != n_features[mod][0]:\n",
    "                    raise ValueError(\"Feature names do not match the number of features for modality \" + mod)\n",
    "\n",
    "        for batch in range(self.nbatches):\n",
    "            # cell number of each batch should be the same\n",
    "            n_cells = np.array([self.Xs[mod][batch].shape[0] for mod in self.Xs.keys() if self.Xs[mod][batch] is not None])\n",
    "            if len(n_cells) > 1:\n",
    "                if np.all(n_cells == n_cells[0]) == False:\n",
    "                    raise ValueError(\"Number of cells not match between modalities for batch \" + str(batch))\n",
    "            \n",
    "            # No all None batch\n",
    "            if np.all(np.array([self.Xs[mod][batch] is None for mod in self.mods]) == True) == True:\n",
    "                raise ValueError(\"Don't have count matrix correspond to \" + str(batch))    \n",
    "            \n",
    "        print(\"Finished.\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def softmax(X: Tensor):\n",
    "        return torch.softmax(X, dim = 1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def recon_loss(X, C1, C2, Sigma, b1, b2):\n",
    "        return (X - C1 @ Sigma @ C2.t() - b1 - b2).pow(2).mean()\n",
    "\n",
    "    @staticmethod\n",
    "    def cosine_loss(A, B):\n",
    "        return -torch.trace(A.t() @ B)/(torch.norm(A) + 1e-6)/(torch.norm(B) + 1e-6)    \n",
    "\n",
    "    def sample_mini_batch(self):\n",
    "        \"\"\"\\\n",
    "        Sample mini batch\n",
    "        \"\"\"\n",
    "        mask_cells = []\n",
    "        mask_feats = []\n",
    "        # sample mini_batch for each cell dimension\n",
    "        for batch in range(self.nbatches):\n",
    "            mask_cells.append(np.random.choice(self.C_cells[str(batch)].shape[0], int(self.C_cells[str(batch)].shape[0] * self.batch_size), replace=False))\n",
    "        \n",
    "        # sample mini_batch for each feature dimension\n",
    "        for mod in self.mods:\n",
    "            mask_feats.append(np.random.choice(self.C_feats[mod].shape[0], int(self.C_feats[mod].shape[0] * self.batch_size), replace=False))\n",
    "        \n",
    "        return mask_cells, mask_feats\n",
    "\n",
    "    def batch_loss(self, mode, alpha, batch_indices = None):\n",
    "        \"\"\"\\\n",
    "            Calculate overall loss term\n",
    "        \"\"\"\n",
    "        # init\n",
    "        loss1 = torch.FloatTensor([0]).to(self.device)\n",
    "        loss2 = torch.FloatTensor([0]).to(self.device)\n",
    "\n",
    "        if mode != 'validation':\n",
    "            mask_cells = batch_indices[\"cells\"]\n",
    "            mask_feats = batch_indices[\"feats\"]\n",
    "            \n",
    "        # reconstruction loss, ||X - scale * C1 @ A_assos @ C2^t - b1 - b2^t||^2    \n",
    "        for batch in range(self.nbatches):\n",
    "            for idx_mod, mod in enumerate(self.mods): # use mods instead of self.mods to reduce computation\n",
    "                if self.Xs[mod][batch] is not None:\n",
    "                    scale = self.scales[mod][batch]\n",
    "                    if mode == \"validation\":\n",
    "                        A_assos = scale * (self.A_assos[\"shared\"] + self.A_assos[mod + \"_\" + str(batch)])\n",
    "                        loss1 += self.recon_loss(self.Xs[mod][batch], self.softmax(self.C_cells[str(batch)]), self.softmax(self.C_feats[mod]), A_assos, self.b_cells[mod][batch], self.b_feats[mod][batch])\n",
    "                        # print(\"loss1_sub: {:.4e}\".format(self.recon_loss(self.Xs[mod][batch], self.softmax(self.C_cells[str(batch)]), self.softmax(self.C_feats[mod]), A_assos, self.b_cells[mod][batch], self.b_feats[mod][batch]).item()) )\n",
    "                    elif (mode == \"C_cells\") or (mode == \"A_assos\") or (mode[:7] == \"C_feats\" and mode[8:] == mod):\n",
    "                        batch_X = self.Xs[mod][batch][np.ix_(mask_cells[batch], mask_feats[idx_mod])]\n",
    "                        batch_C_cells = self.C_cells[str(batch)][mask_cells[batch],:]\n",
    "                        batch_C_feats = self.C_feats[mod][mask_feats[idx_mod], :]\n",
    "                        batch_A_asso = scale * (self.A_assos[mod + \"_\" + str(batch)] + self.A_assos[\"shared\"])\n",
    "                        batch_b_cells = self.b_cells[mod][batch][mask_cells[batch],:]\n",
    "                        batch_b_feats = self.b_feats[mod][batch][:, mask_feats[idx_mod]]\n",
    "                        # check if relu can actually be used, if initialize A_asso to be negative, then A_asso will never be updated as positive\n",
    "                        loss1 += self.recon_loss(batch_X, self.softmax(batch_C_cells), self.softmax(batch_C_feats), batch_A_asso, batch_b_cells, batch_b_feats)\n",
    "                        del batch_X, batch_C_cells, batch_C_feats, batch_A_asso, batch_b_cells, batch_b_feats       \n",
    "\n",
    "\n",
    "        # association loss, calculate when mode is \"validation\" or \"A_assos\"\n",
    "        if (mode == \"validation\") or (mode == \"A_assos\"):\n",
    "            for batch in range(self.nbatches):\n",
    "                for idx_mod, mod in enumerate(self.mods):\n",
    "                    # make sure the l2 norm is minimized\n",
    "                    if self.Xs[mod][batch] is not None:\n",
    "                        loss2 += self.A_assos[mod + \"_\" + str(batch)].pow(2).sum()              \n",
    "      \n",
    "        loss = alpha[0] * loss1 + alpha[1] * loss2 \n",
    "\n",
    "        return loss, loss1, loss2\n",
    "    \n",
    "\n",
    "    def train_func(self, T):\n",
    "        best_loss = 1e12\n",
    "        count = 0\n",
    "        losses = []\n",
    "\n",
    "        for t in range(T):\n",
    "            mask_cells, mask_feats = self.sample_mini_batch()\n",
    "            \n",
    "            # update C_cells\n",
    "            # print(\"update C_cells...\")\n",
    "            self.A_assos[\"shared\"].requires_grad = False\n",
    "            for i, mod in enumerate(self.mods):\n",
    "                self.C_feats[mod].requires_grad = False\n",
    "            for idx in self.A_assos.keys():\n",
    "                self.A_assos[idx].requires_grad = False\n",
    "                        \n",
    "            for batch in range(self.nbatches):\n",
    "                self.C_cells[str(batch)].requires_grad = True\n",
    "\n",
    "            # sanity check\n",
    "            for idx in self.C_cells.keys():\n",
    "                assert self.C_cells[idx].requires_grad\n",
    "            for idx in self.C_feats.keys():\n",
    "                assert not self.C_feats[idx].requires_grad\n",
    "            for idx in self.A_assos.keys():\n",
    "                assert not self.A_assos[idx].requires_grad\n",
    "\n",
    "            # update gradient    \n",
    "            loss, *_ = self.batch_loss(mode = \"C_cells\", alpha = self.alpha, batch_indices = {\"cells\": mask_cells, \"feats\": mask_feats})\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # update C_feats\n",
    "            # print(\"update C_feats...\")\n",
    "            for batch in range(self.nbatches):\n",
    "                self.C_cells[str(batch)].requires_grad = False\n",
    "            for i, mod in enumerate(self.mods):\n",
    "                self.C_feats[mod].requires_grad = True\n",
    "\n",
    "                # sanity check\n",
    "                for idx in self.C_cells.keys():\n",
    "                    assert not self.C_cells[idx].requires_grad\n",
    "                for idx in self.C_feats.keys():\n",
    "                    if idx != mod:\n",
    "                        assert not self.C_feats[idx].requires_grad\n",
    "                    elif idx == mod:\n",
    "                        assert self.C_feats[idx].requires_grad\n",
    "                for idx in self.A_assos.keys():\n",
    "                    assert not self.A_assos[idx].requires_grad\n",
    "\n",
    "                # update gradient  \n",
    "                loss, *_ = self.batch_loss(mode = \"C_feats_\" + mod, alpha = self.alpha, batch_indices = {\"cells\": mask_cells, \"feats\": mask_feats})\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                # update only one C_feats a time\n",
    "                self.C_feats[mod].requires_grad = False\n",
    "\n",
    "            # update A_assos:\n",
    "            # print(\"update A_assos...\")\n",
    "            for idx in self.A_assos.keys():\n",
    "                self.A_assos[idx].requires_grad = True\n",
    "\n",
    "            # sanity check\n",
    "            for idx in self.C_cells.keys():\n",
    "                assert not self.C_cells[idx].requires_grad\n",
    "            for idx in self.C_feats.keys():\n",
    "                assert not self.C_feats[idx].requires_grad\n",
    "            for idx in self.A_assos.keys():\n",
    "                assert self.A_assos[idx].requires_grad\n",
    "\n",
    "            loss, *_ = self.batch_loss(mode = \"A_assos\", alpha = self.alpha, batch_indices = {\"cells\": mask_cells, \"feats\": mask_feats})                    \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # no non-negative constraint\n",
    "            with torch.no_grad():\n",
    "                self.A_assos[\"shared\"].data = self.A_assos[\"shared\"] * (self.A_assos[\"shared\"] > 0)\n",
    "            \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in range(self.nbatches):\n",
    "                    for idx_mod, mod in enumerate(self.mods):\n",
    "                        if self.Xs[mod][batch] is not None:\n",
    "                            batch_X = self.Xs[mod][batch][np.ix_(mask_cells[batch], mask_feats[idx_mod])]\n",
    "                            batch_C_cells = self.C_cells[str(batch)][mask_cells[batch],:]\n",
    "                            batch_C_feats = self.C_feats[mod][mask_feats[idx_mod], :]\n",
    "                            batch_A_asso = self.A_assos[mod + \"_\" + str(batch)] + self.A_assos[\"shared\"]\n",
    "                            batch_b_feats = self.b_feats[mod][batch][:, mask_feats[idx_mod]]\n",
    "                            batch_b_cells = self.b_cells[mod][batch][mask_cells[batch],:]\n",
    "                            # update scale term:\n",
    "                            scale = torch.trace((batch_X - batch_b_cells - batch_b_feats).t() @ (self.softmax(batch_C_cells) @ batch_A_asso @ self.softmax(batch_C_feats).t()))\n",
    "                            scale = scale/(torch.trace((self.softmax(batch_C_cells) @ batch_A_asso @ self.softmax(batch_C_feats).t()).t() @ (self.softmax(batch_C_cells) @ batch_A_asso @ self.softmax(batch_C_feats).t())))\n",
    "                            self.scales[mod][batch] = scale\n",
    "                            # update bias term:\n",
    "                            batch_b_feats = self.b_feats[mod][batch][:, mask_feats[idx_mod]]\n",
    "                            self.b_cells[mod][batch][mask_cells[batch], :] = torch.mean(batch_X - self.scales[mod][batch] * self.softmax(batch_C_cells) @ batch_A_asso @ self.softmax(batch_C_feats).t() - batch_b_feats, dim = 1)[:,None]\n",
    "                            batch_b_cells = self.b_cells[mod][batch][mask_cells[batch],:]\n",
    "                            self.b_feats[mod][batch][:, mask_feats[idx_mod]] = torch.mean(batch_X - self.scales[mod][batch] * self.softmax(batch_C_cells) @ batch_A_asso @ self.softmax(batch_C_feats).t() - batch_b_cells, dim = 0)[None,:]\n",
    "\n",
    "\n",
    "            \n",
    "            # validation       \n",
    "            if ((t+1) % self.interval == 0) | (t == 0):\n",
    "                with torch.no_grad():\n",
    "                    loss, loss1, loss2 = self.batch_loss(mode = \"validation\", alpha = self.alpha)\n",
    "\n",
    "                    # print(self.A_assos[\"shared\"])\n",
    "                    # print(self.A_assos[\"rna_0\"])\n",
    "                    # print(self.A_assos[\"rna_1\"])\n",
    "                \n",
    "                print('Epoch {}, Validating Loss: {:.4f}'.format(t + 1, loss.item()))\n",
    "                info = [\n",
    "                    'loss reconstruction: {:.5f}'.format(loss1.item()),\n",
    "                    'loss regularization: {:.5f}'.format(loss2.item())\n",
    "                ]\n",
    "                for i in info:\n",
    "                    print(\"\\t\", i)\n",
    "                \n",
    "                losses.append(loss.item())\n",
    "                \n",
    "                '''\n",
    "                # update for early stopping \n",
    "                if loss.item() < best_loss:# - 0.01 * abs(best_loss):\n",
    "                    \n",
    "                    best_loss = loss.item()\n",
    "                    # should save the whole model instead of just the state dict\n",
    "                    # torch.save(self.state_dict(), f'../check_points/real_{self.N_cell}.pt')\n",
    "                    count = 0\n",
    "                else:\n",
    "                    count += 1\n",
    "                    print(count)\n",
    "                    if count % int(T/self.interval) == 0:\n",
    "                        self.optimizer.param_groups[0]['lr'] *= 0.5\n",
    "                        print('Epoch: {}, shrink lr to {:.4f}'.format(t + 1, self.optimizer.param_groups[0]['lr']))\n",
    "                        if self.optimizer.param_groups[0]['lr'] < 1e-6:\n",
    "                            break\n",
    "                        else:\n",
    "                            self.load_state_dict(torch.load(f'../check_points/real_{self.N_cell}.pt'))\n",
    "                            count = 0                            \n",
    "                '''\n",
    "        return losses                            \n",
    "\n",
    "\n",
    "\n",
    "def quantile_norm(X):\n",
    "    \"\"\"Normalize the columns of X to each have the same distribution.\n",
    "\n",
    "    Given an expression matrix (microarray data, read counts, etc) of M genes\n",
    "    by N samples, quantile normalization ensures all samples have the same\n",
    "    spread of data (by construction).\n",
    "\n",
    "    The data across each row are averaged to obtain an average column. Each\n",
    "    column quantile is replaced with the corresponding quantile of the average\n",
    "    column.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 2D array of float, shape (M, N)\n",
    "        The input data, with M rows (genes/features) and N columns (samples).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Xn : 2D array of float, shape (M, N)\n",
    "        The normalized data.\n",
    "    \"\"\"\n",
    "    # compute the quantiles\n",
    "    quantiles = np.mean(np.sort(X, axis=0), axis=1)\n",
    "\n",
    "    # compute the column-wise ranks. Each observation is replaced with its\n",
    "    # rank in that column: the smallest observation is replaced by 1, the\n",
    "    # second-smallest by 2, ..., and the largest by M, the number of rows.\n",
    "    ranks = np.apply_along_axis(stats.rankdata, 0, X)\n",
    "\n",
    "    # convert ranks to integer indices from 0 to M-1\n",
    "    rank_indices = ranks.astype(int) - 1\n",
    "\n",
    "    # index the quantiles for each rank with the ranks matrix\n",
    "    Xn = quantiles[rank_indices]\n",
    "\n",
    "    return(Xn)\n",
    "\n",
    "def quantile_norm_log(X, log = True):\n",
    "    if log:\n",
    "        logX = np.log1p(X)\n",
    "    else:\n",
    "        logX = X\n",
    "    logXn = quantile_norm(logX)\n",
    "    return logXn\n",
    "\n",
    "\n",
    "def momat_preprocess(counts, modality = \"rna\", log = True):\n",
    "    \"\"\"\\\n",
    "    Description:\n",
    "    ------------\n",
    "    Preprocess the dataset, for count, interaction matrices\n",
    "    \"\"\"\n",
    "    if modality == \"atac\":\n",
    "        # make binary, maximum is 1\n",
    "        counts = (counts > 0).astype(float) \n",
    "        # # normalize according to library size\n",
    "        # counts = counts / np.sum(counts, axis = 1)[:,None]\n",
    "        # counts = counts/np.max(counts)\n",
    "\n",
    "    else:\n",
    "        # other cases, e.g. Protein, RNA, etc\n",
    "        counts = quantile_norm_log(counts, log = log)\n",
    "        counts = counts/np.max(counts)\n",
    "\n",
    "    return counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = pj(\"result\", \"comparison\", o.task, o.method)\n",
    "data_dir = pj(\"data\", \"processed\", o.task)\n",
    "cfg_task = re.sub(\"_atlas|_generalize|_transfer|_ref_.*\", \"\", o.task) # dogma_full\n",
    "data_config = utils.load_toml(\"configs/data.toml\")[cfg_task]\n",
    "for k, v in data_config.items():\n",
    "    vars(o)[k] = v\n",
    "model_config = utils.load_toml(\"configs/model.toml\")[\"default\"]\n",
    "if o.model != \"default\":\n",
    "    model_config.update(utils.load_toml(\"configs/model.toml\")[o.model])\n",
    "for k, v in model_config.items():\n",
    "    vars(o)[k] = v\n",
    "o.s_joint, o.combs, *_ = utils.gen_all_batch_ids(o.s_joint, o.combs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading predicted variables ...\n",
      "Loading subset 0: x, atac\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:08<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subset 1: x, rna\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:03<00:00,  9.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subset 2: x, adt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:00<00:00, 43.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subset 3: x, atac\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:10<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subset 3: x, rna\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:03<00:00, 11.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subset 3: x, adt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:06<00:00,  5.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to numpy ...\n",
      "Converting subset 0: x, atac\n",
      "Converting subset 1: x, rna\n",
      "Converting subset 2: x, adt\n",
      "Converting subset 3: x, atac\n",
      "Converting subset 3: x, rna\n",
      "Converting subset 3: x, adt\n"
     ]
    }
   ],
   "source": [
    "# Load input\n",
    "o.mods = [\"atac\", \"rna\", \"adt\"]\n",
    "o.pred_dir = pj(\"result\", o.task, o.experiment, o.model, \"predict\", o.init_model)\n",
    "pred = utils.load_predicted(o, joint_latent = False, input=True, group_by = \"subset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get counts and masks\n",
    "counts = {\"atac\":[], \"rna\": [], \"adt\": []}\n",
    "masks = {\"rna\": [], \"adt\": []}\n",
    "for batch_id in pred.keys():\n",
    "    for m in counts.keys():\n",
    "        if m in pred[batch_id][\"x\"].keys():\n",
    "            counts[m].append(momat_preprocess(pred[batch_id][\"x\"][m], modality = m))\n",
    "            if m != \"atac\":\n",
    "                mask_dir = pj(data_dir, \"subset_\"+str(batch_id), \"mask\")\n",
    "                mask = np.array(utils.load_csv(pj(mask_dir, m+\".csv\"))[1][1:]).astype(bool)\n",
    "                masks[m].append(mask)\n",
    "        else:\n",
    "            counts[m].append(None)\n",
    "\n",
    "counts[\"nbatches\"] = len(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature intersection\n",
    "for m in masks.keys():\n",
    "    mask = np.array(masks[m]).prod(axis=0).astype(bool)\n",
    "    for i, count in enumerate(counts[m]):\n",
    "        if count is not None:\n",
    "            counts[m][i] = count[:, mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sanity check...\n",
      "Finished.\n",
      "Epoch 1, Validating Loss: 296.1292\n",
      "\t loss reconstruction: 0.29551\n",
      "\t loss regularization: 0.61440\n",
      "Epoch 1000, Validating Loss: 232.7159\n",
      "\t loss reconstruction: 0.23269\n",
      "\t loss regularization: 0.02851\n",
      "Epoch 2000, Validating Loss: 230.9309\n",
      "\t loss reconstruction: 0.23090\n",
      "\t loss regularization: 0.03392\n",
      "Epoch 3000, Validating Loss: 230.1502\n",
      "\t loss reconstruction: 0.23013\n",
      "\t loss regularization: 0.01567\n",
      "Epoch 4000, Validating Loss: 229.7315\n",
      "\t loss reconstruction: 0.22970\n",
      "\t loss regularization: 0.02919\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "\n",
    "lamb = 0.001\n",
    "batchsize = 0.1\n",
    "seed = 0\n",
    "K = 32\n",
    "interval = 1000\n",
    "T = 4000\n",
    "lr = 1e-2\n",
    "\n",
    "model1 = scmomat(counts = counts, K = K, batch_size = batchsize, interval = interval, lr = lr, lamb = lamb, seed = seed)\n",
    "losses1 = model1.train_func(T = T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings\n",
    "zs = []\n",
    "for batch in range(len(pred)):\n",
    "    z = model1.softmax(model1.C_cells[str(batch)].cpu().detach()).numpy()\n",
    "    zs.append(z)\n",
    "Z = np.concatenate(zs, axis = 0)\n",
    "\n",
    "utils.mkdirs(result_dir, remove_old=False)\n",
    "utils.save_tensor_to_csv(Z, pj(result_dir, 'embeddings.csv'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "61a57a4b5406d2de388e2f91097d4e4bcd7d5f4a46f53a795aa28a02eed27fc5"
  },
  "kernelspec": {
   "display_name": "R",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
