{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/root/workspace/code/sc-transformer/\")\n",
    "from os.path import join as pj\n",
    "import argparse\n",
    "import sys\n",
    "sys.path.append(\"modules\")\n",
    "import utils\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import scib\n",
    "import scib.metrics as me\n",
    "import anndata as ad\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import re\n",
    "import itertools\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score\n",
    "from scipy.stats import pearsonr\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--task', type=str, default='dogma_single_rna_transfer')\n",
    "parser.add_argument('--reference', type=str, default='atlas_no_dogma')\n",
    "parser.add_argument('--experiment', type=str, default='e0')\n",
    "parser.add_argument('--model', type=str, default='default')\n",
    "parser.add_argument('--init_model', type=str, default='sp_00001999')\n",
    "parser.add_argument('--init_model_ref', type=str, default='sp_latest')\n",
    "parser.add_argument('--method', type=str, default='midas_embed')\n",
    "o, _ = parser.parse_known_args()  # for python interactive\n",
    "# o = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_dir = pj(\"result\", \"comparison\", o.task, o.method, o.experiment, o.init_model)\n",
    "\n",
    "# Load latent variables of query data\n",
    "cfg_task = re.sub(\"_atlas|_generalize|_transfer|_ref_.*\", \"\", o.task)\n",
    "data_config = utils.load_toml(\"configs/data.toml\")[cfg_task]\n",
    "for k, v in data_config.items():\n",
    "    vars(o)[k] = v\n",
    "model_config = utils.load_toml(\"configs/model.toml\")[\"default\"]\n",
    "if o.model != \"default\":\n",
    "    model_config.update(utils.load_toml(\"configs/model.toml\")[o.model])\n",
    "for k, v in model_config.items():\n",
    "    vars(o)[k] = v\n",
    "o.s_joint, o.combs, *_ = utils.gen_all_batch_ids(o.s_joint, o.combs)\n",
    "\n",
    "o.pred_dir = pj(\"result\", o.task, o.experiment, o.model, \"predict\", o.init_model)\n",
    "pred = utils.load_predicted(o, group_by=\"subset\")\n",
    "\n",
    "c = [v[\"z\"][\"joint\"][:, :o.dim_c] for v in pred.values()]\n",
    "c = np.concatenate(c, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load latent variables of reference data\n",
    "o_ref = copy.deepcopy(o)\n",
    "o_ref.task = o.reference\n",
    "data_config = utils.load_toml(\"configs/data.toml\")[o_ref.task]\n",
    "for k, v in data_config.items():\n",
    "    vars(o_ref)[k] = v\n",
    "o_ref.s_joint, o_ref.combs, *_ = utils.gen_all_batch_ids(o_ref.s_joint, o_ref.combs)\n",
    "\n",
    "o_ref.pred_dir = pj(\"result\", o_ref.task, o.experiment, o.model, \"predict\", o.init_model_ref)\n",
    "pred_ref = utils.load_predicted(o_ref, group_by=\"subset\")\n",
    "\n",
    "c_ref = [v[\"z\"][\"joint\"][:, :o_ref.dim_c] for v in pred_ref.values()]\n",
    "c_ref = np.concatenate(c_ref, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load labels\n",
    "label_atlas = utils.load_csv(pj(\"result\", \"downstream\", \"labels\", \"labels2.atlas.csv\"))\n",
    "label_gt = np.array(utils.transpose_list(label_atlas)[1][1:])[:len(c)]\n",
    "label_ref = np.array(utils.transpose_list(label_atlas)[1][1:])[len(c):]\n",
    "\n",
    "# label_atlas_no_dogma = utils.load_csv(pj(\"result\", \"downstream\", \"labels\", \"labels2.\"+o.reference+\".csv\"))\n",
    "# label_ref = np.array(utils.transpose_list(label_atlas_no_dogma)[1][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer labels via knn\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "knn.fit(c_ref, label_ref)\n",
    "label_pred = knn.predict(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_gt_keys = np.unique(label_gt)\n",
    "label_pred_keys = np.unique(label_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sp_00003699  f1:  0.32764215314632295\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "results[\"confusion\"] = confusion_matrix(label_gt, label_pred, labels=label_gt_keys)\n",
    "results[\"f1\"] = f1_score(label_gt, label_pred, average='micro')\n",
    "print(o.task, o.init_model, \" f1: \", results[\"f1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plt.figure(figsize=(20,17))\n",
    "# sns.set(font_scale=1.5)\n",
    "# cm = results[\"confusion\"].astype('float') / results[\"confusion\"].sum(axis=1)[:, np.newaxis]\n",
    "# ax = sns.heatmap(cm, annot=True, annot_kws={\"size\": 16})\n",
    "# ax.xaxis.set_ticklabels(label_gt_keys, rotation=45)\n",
    "# ax.yaxis.set_ticklabels(label_gt_keys, rotation=45)\n",
    "# plt.title(o.task)\n",
    "# # plt.savefig(pj(fig_dir, \"confusion_\"+o.data+\"_\"+task+\".png\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_gt = utils.load_csv(pj(\"result\", \"downstream\", \"labels\", \"labels2.atlas.csv\"))\n",
    "# label_gt = np.array(utils.transpose_list(label_gt)[1][1:])[len(c):]\n",
    "# label_gt_keys = np.unique(label_gt)\n",
    "# results[\"confusion\"] = confusion_matrix(label_gt, label_ref, labels=label_gt_keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# plt.figure(figsize=(20,17))\n",
    "# sns.set(font_scale=1.5)\n",
    "# cm = results[\"confusion\"].astype('float') / results[\"confusion\"].sum(axis=1)[:, np.newaxis]\n",
    "# ax = sns.heatmap(cm, annot=True, annot_kws={\"size\": 16})\n",
    "# ax.xaxis.set_ticklabels(label_gt_keys, rotation=45)\n",
    "# ax.yaxis.set_ticklabels(label_gt_keys, rotation=45)\n",
    "# # plt.savefig(pj(fig_dir, \"confusion_\"+o.data+\"_\"+task+\".png\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# for batch_id in pred.keys():\n",
    "    \n",
    "#     print(\"Processing batch: \", batch_id)\n",
    "#     z = pred[batch_id][\"z\"]\n",
    "\n",
    "\n",
    "#     c = {m: v[:, :o.dim_c] for m, v in z.items()}\n",
    "#     c_cat = np.concatenate((c[\"atac\"], c[\"rna\"], c[\"adt\"]), axis=0)\n",
    "#     mods_cat = [\"atac\"]*len(c[\"atac\"]) + [\"rna\"]*len(c[\"rna\"]) + [\"adt\"]*len(c[\"adt\"])\n",
    "    \n",
    "#     label = utils.load_csv(pj(o.raw_data_dirs[batch_id], \"label_seurat\", \"l1.csv\"))\n",
    "#     label = np.array(utils.transpose_list(label)[1][1:])\n",
    "#     label_cat = np.tile(label, 3)\n",
    "    \n",
    "#     assert len(c_cat) == len(mods_cat) == len(label_cat), \"Inconsistent lengths!\"\n",
    "    \n",
    "#     batch = str(batch_id) # toml dict key must be str\n",
    "\n",
    "\n",
    "#     results[\"f1\"][batch] = {}\n",
    "\n",
    "#     print(\"Computing f1\")\n",
    "#     knn.fit(c_ref[\"joint\"], label_ref[\"joint\"])\n",
    "#     label_pred = knn.predict(c[\"joint\"])\n",
    "#     # cm = confusion_matrix(label_gt, label_pred, labels=knn.classes_)\n",
    "#     results[\"f1\"][batch] = f1_score(label_gt, label_pred, average='micro')\n",
    "#     # f1_weighted = f1_score(label_gt, label_pred, average='weighted')\n",
    "    \n",
    "\n",
    "# utils.mkdirs(result_dir, remove_old=False)\n",
    "# utils.save_toml(results, pj(result_dir, \"metrics_mod_detailed.toml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_avg = {metric: np.mean(utils.extract_values(v)) for metric, v in results.items()}\n",
    "# df = pd.DataFrame({\n",
    "#     'Label_transfer':   [results_avg['f1']],\n",
    "# })\n",
    "# print(df)\n",
    "# df.to_excel(pj(result_dir, \"metrics_mod.xlsx\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "61a57a4b5406d2de388e2f91097d4e4bcd7d5f4a46f53a795aa28a02eed27fc5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
