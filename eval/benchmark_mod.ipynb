{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/root/workspace/code/sc-transformer/\")\n",
    "from os.path import join as pj\n",
    "import argparse\n",
    "import sys\n",
    "sys.path.append(\"modules\")\n",
    "import utils\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import scib\n",
    "import scib.metrics as me\n",
    "import anndata as ad\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import re\n",
    "import itertools\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--task', type=str, default='dogma_paired_full')\n",
    "parser.add_argument('--experiment', type=str, default='e0')\n",
    "parser.add_argument('--model', type=str, default='default')\n",
    "parser.add_argument('--init_model', type=str, default='sp_latest')\n",
    "parser.add_argument('--method', type=str, default='midas_embed')\n",
    "o, _ = parser.parse_known_args()  # for python interactive\n",
    "# o = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"midas\" in o.method:\n",
    "    result_dir = pj(\"result\", \"comparison\", o.task, o.method, o.experiment, o.init_model)\n",
    "else:\n",
    "    result_dir = pj(\"result\", \"comparison\", o.task, o.method)\n",
    "t = o.task.split(\"_\")[0] # dogma\n",
    "o.task = re.sub(t, t+\"_full_ref\", o.task) # dogma_full_ref_paired_full\n",
    "# data_dir = pj(\"data\", \"processed\", re.sub(\"_generalize\", \"_transfer\", o.task))\n",
    "data_dir = pj(\"data\", \"processed\", o.task)\n",
    "cfg_task = re.sub(\"_atlas|_generalize|_transfer|_ref_.*\", \"\", o.task) # dogma_full\n",
    "data_config = utils.load_toml(\"configs/data.toml\")[cfg_task]\n",
    "for k, v in data_config.items():\n",
    "    vars(o)[k] = v\n",
    "model_config = utils.load_toml(\"configs/model.toml\")[\"default\"]\n",
    "if o.model != \"default\":\n",
    "    model_config.update(utils.load_toml(\"configs/model.toml\")[o.model])\n",
    "for k, v in model_config.items():\n",
    "    vars(o)[k] = v\n",
    "o.s_joint, o.combs, *_ = utils.gen_all_batch_ids(o.s_joint, o.combs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading predicted variables ...\n"
     ]
    }
   ],
   "source": [
    "# Load predicted latent variables\n",
    "o.mods = [\"atac\", \"rna\", \"adt\"]\n",
    "o.pred_dir = pj(\"result\", o.task, o.experiment, o.model, \"predict\", o.init_model)\n",
    "pred = utils.load_predicted(o, mod_latent=True, translate=True, input=True, group_by=\"subset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_type = \"embed\"\n",
    "embed = \"X_emb\"\n",
    "batch_key = \"batch\"\n",
    "label_key = \"label\"\n",
    "mod_key = \"modality\"\n",
    "cluster_key = \"cluster\"\n",
    "si_metric = \"euclidean\"\n",
    "subsample = 0.5\n",
    "verbose = False\n",
    "\n",
    "results = {\n",
    "    \"asw_mod\": {},\n",
    "    \"foscttm\": {},\n",
    "    \"f1\": {},\n",
    "    \"auroc\": {},\n",
    "    \"pearson_rna\": {},\n",
    "    \"pearson_adt\": {},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "\n",
    "for batch_id in pred.keys():\n",
    "    \n",
    "    print(\"Processing batch: \", batch_id)\n",
    "    z = pred[batch_id][\"z\"]\n",
    "    x = pred[batch_id][\"x\"]\n",
    "    x_trans = pred[batch_id][\"x_trans\"]\n",
    "    mask_dir = pj(data_dir, \"subset_\"+str(batch_id), \"mask\")\n",
    "\n",
    "    c = {m: v[:, :o.dim_c] for m, v in z.items()}\n",
    "    c_cat = np.concatenate((c[\"atac\"], c[\"rna\"], c[\"adt\"]), axis=0)\n",
    "    mods_cat = [\"atac\"]*len(c[\"atac\"]) + [\"rna\"]*len(c[\"rna\"]) + [\"adt\"]*len(c[\"adt\"])\n",
    "    \n",
    "    label = utils.load_csv(pj(o.raw_data_dirs[batch_id], \"label_seurat\", \"l1.csv\"))\n",
    "    label = np.array(utils.transpose_list(label)[1][1:])\n",
    "    label_cat = np.tile(label, 3)\n",
    "    \n",
    "    assert len(c_cat) == len(mods_cat) == len(label_cat), \"Inconsistent lengths!\"\n",
    "    \n",
    "    batch = str(batch_id) # toml dict key must be str\n",
    "    print(\"Computing asw_mod\")\n",
    "    adata = ad.AnnData(c_cat)\n",
    "    adata.obsm[embed] = c_cat\n",
    "    adata.obs[mod_key] = mods_cat\n",
    "    adata.obs[mod_key] = adata.obs[mod_key].astype(\"category\")\n",
    "    adata.obs[label_key] = label_cat\n",
    "    adata.obs[label_key] = adata.obs[label_key].astype(\"category\")\n",
    "    results[\"asw_mod\"][batch] = me.silhouette_batch(adata, batch_key=mod_key,\n",
    "        group_key=label_key, embed=embed, metric=si_metric, verbose=verbose)\n",
    "    \n",
    "    results[\"foscttm\"][batch] = {}\n",
    "    results[\"f1\"][batch] = {}\n",
    "    results[\"auroc\"][batch] = {}\n",
    "    results[\"pearson_rna\"][batch] = {}\n",
    "    results[\"pearson_adt\"][batch] = {}\n",
    "    for m in c.keys() - {\"joint\"}:\n",
    "        for m_ in set(c.keys()) - {m, \"joint\"}:\n",
    "            k = m+\"_to_\"+m_\n",
    "            print(k+\":\")\n",
    "            print(\"Computing foscttm\")\n",
    "            results[\"foscttm\"][batch][k] = 1 - utils.calc_foscttm(th.from_numpy(c[m]), th.from_numpy(c[m_]))\n",
    "            \n",
    "            print(\"Computing f1\")\n",
    "            knn.fit(c[m], label)\n",
    "            label_pred = knn.predict(c[m_])\n",
    "            # cm = confusion_matrix(label, label_pred, labels=knn.classes_)\n",
    "            results[\"f1\"][batch][k] = f1_score(label, label_pred, average='micro')\n",
    "            # f1_weighted = f1_score(label, label_pred, average='weighted')\n",
    "            \n",
    "            if m_ in [\"atac\"]:\n",
    "                x_gt = x[m_].reshape(-1)\n",
    "                x_pred = x_trans[k].reshape(-1)\n",
    "                print(\"Computing auroc\")\n",
    "                results[\"auroc\"][batch][k] = roc_auc_score(x_gt, x_pred)\n",
    "            elif m_ in [\"rna\", \"adt\"]:\n",
    "                mask = np.array(utils.load_csv(pj(mask_dir, m_+\".csv\"))[1][1:]).astype(bool)\n",
    "                x_gt = x[m_][:, mask].reshape(-1)\n",
    "                x_pred = x_trans[k][:, mask].reshape(-1)\n",
    "                print(\"Computing pearson_\"+m_)\n",
    "                results[\"pearson_\"+m_][batch][k] = pearsonr(x_gt, x_pred)[0]\n",
    "                \n",
    "    for mods in itertools.combinations(c.keys() - {\"joint\"}, 2): # double to single\n",
    "        m1, m2 = utils.ref_sort(mods, ref=o.mods)\n",
    "        m_ = list(set(o.mods) - set(mods))[0]\n",
    "        k = m1+\"_\"+m2+\"_to_\"+m_\n",
    "        print(k+\":\")\n",
    "        \n",
    "        if m_ in [\"atac\"]:\n",
    "            x_gt = x[m_].reshape(-1)\n",
    "            x_pred = x_trans[k].reshape(-1)\n",
    "            print(\"Computing auroc\")\n",
    "            results[\"auroc\"][batch][k] = roc_auc_score(x_gt, x_pred)\n",
    "        elif m_ in [\"rna\", \"adt\"]:\n",
    "            mask = np.array(utils.load_csv(pj(mask_dir, m_+\".csv\"))[1][1:]).astype(bool)\n",
    "            x_gt = x[m_][:, mask].reshape(-1)\n",
    "            x_pred = x_trans[k][:, mask].reshape(-1)\n",
    "            print(\"Computing pearson_\"+m_)\n",
    "            results[\"pearson_\"+m_][batch][k] = pearsonr(x_gt, x_pred)[0]\n",
    "\n",
    "utils.mkdirs(result_dir, remove_old=False)\n",
    "utils.save_toml(results, pj(result_dir, \"metrics_mod_detailed.toml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_avg = {metric: np.mean(utils.extract_values(v)) for metric, v in results.items()}\n",
    "df = pd.DataFrame({\n",
    "    'ASW_mod':          [results_avg['asw_mod']],\n",
    "    'FOSCTTM':          [results_avg['foscttm']],\n",
    "    'Label_transfer':   [results_avg['f1']],\n",
    "    'AUROC':            [results_avg['auroc']],\n",
    "    'Pearson_RNA':      [results_avg['pearson_rna']],\n",
    "    'Pearson_ADT':      [results_avg['pearson_adt']],\n",
    "})\n",
    "print(df)\n",
    "df.to_excel(pj(result_dir, \"metrics_mod.xlsx\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "61a57a4b5406d2de388e2f91097d4e4bcd7d5f4a46f53a795aa28a02eed27fc5"
  },
  "kernelspec": {
   "display_name": "R",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11 (default, Jul 27 2021, 14:32:16) \n[GCC 7.5.0]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
