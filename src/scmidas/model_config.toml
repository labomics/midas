# Basic configuration
[default]
dim_c = 32  # Latent dimension for biological information
dim_u = 2   # Latent dimension for technical information (always be small to avoid capturing biological information)

# Loss function weights
lam_kld_c = 1         # Weight for KLD loss (for c)
lam_kld_u = 5         # Weight for KLD loss (for u)
lam_kld = 1           # Weight for total KLD loss
lam_recon = 1         # Weight for reconstruction loss
lam_dsc = 30          # Weight for discriminator loss (for adversarial learning)
lam_adv = 1           # Weight for adversarial loss
lam_alignment = 50    # Weight for modality alignment loss

# Discriminator iteration
n_iter_disc = 3  # Number of discriminator iterations before training the VAE

# Basic network structure (MLP)
norm = "ln"           # Use layer normalization
drop = 0.2            # Dropout rate
out_trans = "mish"    # Activation function for the output

# Modality configuration
dims_shared_enc = [1024, 128]  # Shared encoder structure across all modalities
dims_shared_dec = [128, 1024]  # Shared decoder structure across all modalities

available_mods = ["rna", "adt", "atac"]  # Supported modalities

# RNA modality configuration
trsf_before_enc_rna = "log1p"      # Apply log1p transformation before encoding. Exponential transformation will be applied after decoding.
distribution_dec_rna = "POISSON"   # Poisson distribution assumption for decoder
lam_recon_rna = 1                  # Weight for RNA reconstruction loss

# ADT modality configuration
trsf_before_enc_adt = "log1p"      # Apply log1p transformation before encoding. Exponential transformation will be applied after decoding.
distribution_dec_adt = "POISSON"   # Poisson distribution assumption for decoder
lam_recon_adt = 1                  # Weight for ADT reconstruction loss

# ATAC modality configuration
dims_before_enc_atac = [512, 128]  # Independent MLP structure before shared encoder
dims_after_dec_atac = [128, 512]   # Independent MLP structure after shared decoder
distribution_dec_atac = "BERNOULLI"  # Bernoulli distribution assumption for decoder. Use BCE loss.
lam_recon_atac = 1                 # Weight for ATAC reconstruction loss

# Batch-related configuration
s_drop_rate = 0.1              # Subsample dropout rate
dims_enc_s = [16, 16]          # Encoder structure for batch effect correction
dims_dec_s = [16, 16]          # Decoder structure for batch effect correction
lam_recon_s = 1000             # Weight for batch effect reconstruction loss
dims_dsc = [128, 64]           # Structure of the discriminator

# Training configuration
optim_net = "AdamW"            # Optimizer for the main network
lr_net = 1e-4                  # Learning rate for the main network
optim_dsc = "AdamW"            # Optimizer for the discriminator
lr_dsc = 1e-4                  # Learning rate for the discriminator
grad_clip = -1                 # Gradient clipping (-1 means no clipping)

# Data loader configuration
num_workers = 20               # Number of worker threads for data loading
pin_memory = true              # Load data into pinned memory
persistent_workers = true      # Persistent worker threads
n_max = 10000                  # Maximum number of samples per batch
